// Run edge probing models over BERT,
// without training an encoder on pre-training tasks.
//
// Use this for baseline probing & hyperparameter tuning for probing models.
//
// You should override bert_model_name with one of the valid BERT models, as
// defined in https://github.com/huggingface/pytorch-pretrained-BERT
// For example:
//    bert-base-uncased
//    bert-large-uncased
//    bert-base-cased   (recommended for NER)
//    bert-large-cased  (recommended for NER)

// This imports the defaults, which can be overridden below.
include "../../defaults.conf"  // relative path to this file

exp_name = "count2vec-glove-ontonotes-coref"  // configure this
run_name = "run"  // default

pretrain_tasks = ""  // empty: don't run main training phase
target_tasks = "edges-coref-ontonotes"

// Eval will use task-specific params.
do_pretrain = 0        // skip main train phase
allow_untrained_encoder_parameters = 1  // allow skipping training phase
allow_missing_task_map = 1  // ignore missing classifier_task_map.json
do_target_task_training = 1  // train using eval task params
do_full_eval = 1
write_preds = "val,test"  // don't do #datascience on the test set

lr_patience = 5  // vals until LR decay
patience = 20      // vals until early-stopping

tokenizer = ""  // use native tokenization
word_embs = "glove"

count2vec = 1  // If true, use Count2Vec embedder
count2vec_weight_file_path = ""  // Path to pretrained weights for Count2Vec encoder
count2vec_collection_dir_path = "/home/mkarutz/cstlm-embed/collections/wikitext-101-1.0"
count2vec_reversed_collection_dir_path = ""

// Use no-op encoder (no params).
sent_enc = "none"
skip_embs = 1  // forward embeddings from lower level.
sep_embs_for_skip = 1  // use task embeddings since we skip the generic ones.
