// Run edge probing models over BERT,
// without training an encoder on pre-training tasks.
//
// Use this for baseline probing & hyperparameter tuning for probing models.
//
// You should override bert_model_name with one of the valid BERT models, as
// defined in https://github.com/huggingface/pytorch-pretrained-BERT
// For example:
//    bert-base-uncased
//    bert-large-uncased
//    bert-base-cased   (recommended for NER)
//    bert-large-cased  (recommended for NER)

// This imports the defaults, which can be overridden below.
include "../../defaults.conf"  // relative path to this file

exp_name = "glove-ontonotes-all"  // configure this
run_name = "run"  // default

pretrain_tasks = ""  // empty: don't run main training phase
target_tasks = "edges-pos-ontonotes,edges-nonterminal-ontonotes,edges-ner-ontonotes,edges-coref-ontonotes,edges-srl-ontonotes"

// Eval will use task-specific params.
do_pretrain = 0        // skip main train phase
allow_untrained_encoder_parameters = 1  // allow skipping training phase
allow_missing_task_map = 1  // ignore missing classifier_task_map.json
do_target_task_training = 1  // train using eval task params
do_full_eval = 1
write_preds = "val,test"  // don't do #datascience on the test set

lr_patience = 5  // vals until LR decay
patience = 20      // vals until early-stopping

tokenizer = ""  // use native tokenization
word_embs = "glove"

// Use no-op encoder (no params).
sent_enc = "none"
skip_embs = 1  // forward embeddings from lower level.
sep_embs_for_skip = 1  // use task embeddings since we skip the generic ones.
